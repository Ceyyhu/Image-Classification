{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ef93b604",
   "metadata": {},
   "source": [
    "We used the \"Birds_25\" dataset for this assignment.\n",
    "In Parts 1 and 2, we changed the image size to 128x128 pixels (it was necessary for memory usage).\n",
    "For Part 3, 4 and 5 we used 224x224 pixels.\n",
    "We divided the data into training, validation, and test parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6df880bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python        : 3.11.4\n",
      "OpenCV        : 4.11.0\n",
      "NumPy         : 1.24.1\n",
      "scikit-learn  : 1.6.1\n",
      "scikit-image  : 0.25.2\n",
      "PyTorch       : 2.5.1+cu121\n",
      "TorchVision   : 0.20.1+cu121\n",
      "tqdm          : 4.67.1\n",
      "Matplotlib    : 3.6.2\n"
     ]
    }
   ],
   "source": [
    "#versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfb1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- PyTorch & TorchVision (CUDA 12.1 build) ---\n",
    "pip install --index-url https://download.pytorch.org/whl/cu121 \\\n",
    "    torch==2.5.1+cu121 \\\n",
    "    torchvision==0.20.1+cu121\n",
    "\n",
    "# --- Remaining libraries (exact versions) ---\n",
    "pip install \\\n",
    "    opencv-python==4.11.0.0 \\\n",
    "    numpy==1.24.1 \\\n",
    "    scikit-learn==1.6.1 \\\n",
    "    scikit-image==0.25.2 \\\n",
    "    tqdm==4.67.1 \\\n",
    "    matplotlib==3.6.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73618958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library \n",
    "import os\n",
    "import time\n",
    "\n",
    "# Third-party: general utilities\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# scikit-learn stack\n",
    "from sklearn.base import clone\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# scikit-image\n",
    "from skimage.feature import hog\n",
    "\n",
    "# PyTorch & TorchVision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from torchvision import datasets, models, transforms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d168a31",
   "metadata": {},
   "source": [
    "#### LOADING THE DATASET FOR PART 1 & 2 & BONUS\n",
    "\n",
    "We load all the dataset resized to 128x128 for speed and memory usage (256x256 size would give memory error.) We used all the dataset with lower resolution and not part of dataset with bigger resolution because we observed that it gave better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e5b061f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset for part 1&2\n",
    "IMAGE_SIZE = (128, 128)\n",
    "\n",
    "def load_images_from_folder(folder):\n",
    "    X, y = [], []\n",
    "    for label in os.listdir(folder):\n",
    "        class_path = os.path.join(folder, label)\n",
    "        for file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, file)\n",
    "            img = cv2.imread(img_path)\n",
    "            if img is not None:\n",
    "                img = cv2.resize(img, IMAGE_SIZE)\n",
    "                X.append(img)\n",
    "                y.append(label)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "train_path = \"Birds_25/train\"\n",
    "valid_path = \"Birds_25/valid\"\n",
    "\n",
    "X_train, y_train = load_images_from_folder(train_path)\n",
    "X_valid, y_valid_full = load_images_from_folder(valid_path)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_valid, y_valid_full, test_size=0.5, stratify=y_valid_full, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c95bd681",
   "metadata": {},
   "source": [
    "### Functions used in Part 1 & 2 & Bonus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6594cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Part1 & 2 & Bonus functions\n",
    "def extract_color_hist(img):\n",
    "    hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV)\n",
    "    hist = cv2.calcHist([hsv], [0, 1, 2], None, [8,8,8], [0,180,0,256,0,256])\n",
    "    return hist.flatten()\n",
    "\n",
    "def extract_hog(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    return hog(gray, pixels_per_cell=(16, 16), cells_per_block=(2, 2), feature_vector=True)\n",
    "\n",
    "def extract_sift(img):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    sift = cv2.SIFT_create()\n",
    "    kp, des = sift.detectAndCompute(gray, None)\n",
    "    if des is None:\n",
    "        return np.zeros(128)\n",
    "    return np.mean(des, axis=0)\n",
    "\n",
    "def extract_features(X, method):\n",
    "    features = []\n",
    "    for img in tqdm(X):\n",
    "        if method == 'color_hist':\n",
    "            features.append(extract_color_hist(img))\n",
    "        elif method == 'hog':\n",
    "            features.append(extract_hog(img))\n",
    "        elif method == 'sift':\n",
    "            features.append(extract_sift(img))\n",
    "    return np.array(features)\n",
    "\n",
    "def train_and_val(X_train, y_train, X_val, y_val, model):\n",
    "    model.fit(X_train, y_train)\n",
    "    preds = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    prec = precision_score(y_val, preds, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_val, preds, average='macro', zero_division=0)\n",
    "    f1 = f1_score(y_val, preds, average='macro', zero_division=0)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "def evaluate(model, X, y):\n",
    "    pred = model.predict(X)\n",
    "    return (\n",
    "        accuracy_score(y, pred),\n",
    "        precision_score(y, pred, average='macro', zero_division=0),\n",
    "        recall_score(y, pred, average='macro', zero_division=0),\n",
    "        f1_score(y, pred, average='macro', zero_division=0),\n",
    "    )\n",
    "\n",
    "#---------------------\n",
    "\n",
    "def reduce_features(method, X_train, X_val, y_train=None, n_components=100):\n",
    "    if method == 'pca':\n",
    "        reducer = PCA(n_components=n_components)\n",
    "    elif method == 'selectkbest':\n",
    "        reducer = SelectKBest(score_func=f_classif, k=n_components)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "    if method == 'selectkbest':\n",
    "        X_train_red = reducer.fit_transform(X_train, y_train)\n",
    "    else:\n",
    "        X_train_red = reducer.fit_transform(X_train)\n",
    "\n",
    "    X_val_red  = reducer.transform(X_val)\n",
    "    return X_train_red, X_val_red, reducer \n",
    "\n",
    "#---------------------\n",
    "\n",
    "#Part1 & 2 variables\n",
    "features_to_try = ['color_hist', 'hog', 'sift']\n",
    "models_to_try = {\n",
    "    'SVM': SVC(),\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'MLP': MLPClassifier(max_iter=500)\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d773b04",
   "metadata": {},
   "source": [
    "### PART 1\n",
    "\n",
    "In this part, we looked at different ways to get features from images and used them with some standard machine learning models.\n",
    "Features we tried: Color Histograms, Histogram of Oriented Gradients (HOG), and SIFT.\n",
    "Models we tried: Support Vector Machine (SVM), Random Forest, and Multilayer Perceptron (MLP)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae1aa75e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30000/30000 [00:02<00:00, 12503.82it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3749/3749 [00:00<00:00, 12795.44it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3750/3750 [00:00<00:00, 13204.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color_hist | SVM → VAL  A:0.4823 P:0.4988 R:0.4823 F1:0.4843 | TEST A:0.4891 P:0.5141 R:0.4891 F1:0.4935\n",
      "color_hist | RandomForest → VAL  A:0.8634 P:0.8659 R:0.8634 F1:0.8634 | TEST A:0.8547 P:0.8567 R:0.8547 F1:0.8547\n",
      "color_hist | MLP → VAL  A:0.7399 P:0.7427 R:0.7399 F1:0.7405 | TEST A:0.7296 P:0.7328 R:0.7296 F1:0.7299\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:24<00:00, 357.03it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:10<00:00, 357.29it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:10<00:00, 353.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hog | SVM → VAL  A:0.3609 P:0.3649 R:0.3609 F1:0.3572 | TEST A:0.3523 P:0.3558 R:0.3523 F1:0.3475\n",
      "hog | RandomForest → VAL  A:0.2601 P:0.2615 R:0.2601 F1:0.2556 | TEST A:0.2536 P:0.2540 R:0.2536 F1:0.2493\n",
      "hog | MLP → VAL  A:0.2577 P:0.2567 R:0.2577 F1:0.2565 | TEST A:0.2272 P:0.2291 R:0.2272 F1:0.2278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:37<00:00, 308.71it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:11<00:00, 315.35it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:12<00:00, 302.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sift | SVM → VAL  A:0.3035 P:0.2983 R:0.3036 F1:0.2965 | TEST A:0.2971 P:0.2970 R:0.2971 F1:0.2932\n",
      "sift | RandomForest → VAL  A:0.2507 P:0.2434 R:0.2507 F1:0.2413 | TEST A:0.2416 P:0.2433 R:0.2416 F1:0.2367\n",
      "sift | MLP → VAL  A:0.2611 P:0.2558 R:0.2611 F1:0.2553 | TEST A:0.2488 P:0.2486 R:0.2488 F1:0.2459\n"
     ]
    }
   ],
   "source": [
    "#Part1\n",
    "\n",
    "for feat in features_to_try:\n",
    "    X_train_feat = extract_features(X_train, feat)\n",
    "    X_val_feat   = extract_features(X_val,   feat)\n",
    "    X_test_feat  = extract_features(X_test,  feat)\n",
    "\n",
    "    for name, base_model in models_to_try.items():\n",
    "        #scale only for models that need it\n",
    "        model = make_pipeline(StandardScaler(), clone(base_model)) \\\n",
    "                if name in (\"SVM\", \"MLP\") else clone(base_model)\n",
    "\n",
    "        model.fit(X_train_feat, y_train)\n",
    "\n",
    "        val_acc, val_prec, val_rec, val_f1   = evaluate(model, X_val_feat,  y_val)\n",
    "        test_acc, test_prec, test_rec, test_f1 = evaluate(model, X_test_feat, y_test)\n",
    "\n",
    "        print(f\"{feat} | {name}\"\n",
    "              f\" → VAL  A:{val_acc:.4f} P:{val_prec:.4f} R:{val_rec:.4f} F1:{val_f1:.4f}\"\n",
    "              f\" | TEST A:{test_acc:.4f} P:{test_prec:.4f} R:{test_rec:.4f} F1:{test_f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edf6948d",
   "metadata": {},
   "source": [
    "### Part 1 Results\n",
    "\n",
    "### Part 1 – Raw handcrafted features\n",
    "\n",
    "| Model | Feature    | Split | Acc        | Prec   | Rec    | F1     |\n",
    "| ----- | ---------- | ----- | ---------- | ------ | ------ | ------ |\n",
    "| SVM   | color hist | VAL   | 0.4823     | 0.4988 | 0.4823 | 0.4843 |\n",
    "| SVM   | color hist | TEST  | 0.4891     | 0.5141 | 0.4891 | 0.4935 |\n",
    "| SVM   | HOG        | VAL   | 0.3609     | 0.3649 | 0.3609 | 0.3572 |\n",
    "| SVM   | HOG        | TEST  | 0.3523     | 0.3558 | 0.3523 | 0.3475 |\n",
    "| SVM   | SIFT       | VAL   | 0.3035     | 0.2983 | 0.3036 | 0.2965 |\n",
    "| SVM   | SIFT       | TEST  | 0.2971     | 0.2970 | 0.2971 | 0.2932 |\n",
    "| RF    | color hist | VAL   | 0.8634     | 0.8659 | 0.8634 | 0.8634 |\n",
    "| RF    | color hist | TEST  | **0.8547** | 0.8567 | 0.8547 | 0.8547 |\n",
    "| RF    | HOG        | VAL   | 0.2601     | 0.2615 | 0.2601 | 0.2556 |\n",
    "| RF    | HOG        | TEST  | 0.2536     | 0.2540 | 0.2536 | 0.2493 |\n",
    "| RF    | SIFT       | VAL   | 0.2507     | 0.2434 | 0.2507 | 0.2413 |\n",
    "| RF    | SIFT       | TEST  | 0.2416     | 0.2433 | 0.2416 | 0.2367 |\n",
    "| MLP   | color hist | VAL   | 0.7399     | 0.7427 | 0.7399 | 0.7405 |\n",
    "| MLP   | color hist | TEST  | 0.7296     | 0.7328 | 0.7296 | 0.7299 |\n",
    "| MLP   | HOG        | VAL   | 0.2577     | 0.2567 | 0.2577 | 0.2565 |\n",
    "| MLP   | HOG        | TEST  | 0.2272     | 0.2291 | 0.2272 | 0.2278 |\n",
    "| MLP   | SIFT       | VAL   | 0.2611     | 0.2558 | 0.2611 | 0.2553 |\n",
    "| MLP   | SIFT       | TEST  | 0.2488     | 0.2486 | 0.2488 | 0.2459 |\n",
    "\n",
    "**Best Part 1 result**\n",
    "`RandomForest + color histogram` – **Test Acc 0.8547**, Prec 0.8567, Rec 0.8547, F1 0.8547.\n",
    "\n",
    "**Comments**\n",
    "\n",
    "* Colour information is highly discriminative for these bird classes.\n",
    "* RandomForest handles the 512-bin histogram superbly; SVM/MLP benefit less.\n",
    "* Texture/shape-only features (HOG, SIFT-mean) perform poorly across models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1916051",
   "metadata": {},
   "source": [
    "### PART 2\n",
    "\n",
    "In this part, we used different ways to get & reduce features from images and used them with some standard machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24aaa46b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PART 2: FEATURE REDUCTION (PCA & SelectKBest) ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30000/30000 [00:02<00:00, 12450.20it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3749/3749 [00:00<00:00, 12673.06it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3750/3750 [00:00<00:00, 12498.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA | color_hist | SVM → VAL (0.45558815684182447, 0.46888496732742824, 0.45561521252796416, 0.456501437947503) | TEST (0.4672, 0.4846156414452627, 0.46720000000000006, 0.46996938467172605)\n",
      "PCA | color_hist | RandomForest → VAL (0.7959455854894638, 0.7986149178347363, 0.7959284116331097, 0.7964245009893173) | TEST (0.7826666666666666, 0.785090827847431, 0.7826666666666666, 0.7828678387221011)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA | color_hist | MLP → VAL (0.6190984262469992, 0.6233526967313631, 0.6191230425055929, 0.6187049263301689) | TEST (0.5933333333333334, 0.5946348817551257, 0.5933333333333333, 0.592242827798504)\n",
      "KBest | color_hist | SVM → VAL (0.38783675646839155, 0.41761989445736125, 0.3878335570469798, 0.39356752061376915) | TEST (0.3864, 0.4168012694641683, 0.3864000000000001, 0.3924319939831774)\n",
      "KBest | color_hist | RandomForest → VAL (0.7967457988797012, 0.8006116485896917, 0.7967266219239373, 0.7974758764403621) | TEST (0.7858666666666667, 0.7889542959002622, 0.7858666666666665, 0.7862688334478087)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (500) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KBest | color_hist | MLP → VAL (0.5398773006134969, 0.5443093037077852, 0.5398747203579418, 0.5404985665232754) | TEST (0.5450666666666667, 0.5514478552103956, 0.5450666666666667, 0.5469690509704026)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:22<00:00, 361.65it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:10<00:00, 363.45it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:10<00:00, 363.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA | hog | SVM → VAL (0.36729794611896505, 0.3681230504830001, 0.36732348993288594, 0.36407602716451615) | TEST (0.3525333333333333, 0.35330240012527847, 0.35253333333333337, 0.3485918752735299)\n",
      "PCA | hog | RandomForest → VAL (0.29101093624966656, 0.28964130154612056, 0.2910514541387025, 0.28713335835583165) | TEST (0.2677333333333333, 0.26487010574748593, 0.2677333333333333, 0.2623671370539299)\n",
      "PCA | hog | MLP → VAL (0.29047746065617497, 0.28574084108665637, 0.2905109619686801, 0.2853721901041897) | TEST (0.2792, 0.27632838560815626, 0.2792, 0.27442189385342175)\n",
      "KBest | hog | SVM → VAL (0.20832221925846892, 0.20741942318545648, 0.20835794183445192, 0.2041001929418394) | TEST (0.20586666666666667, 0.20722003873055703, 0.2058666666666667, 0.2016435692999753)\n",
      "KBest | hog | RandomForest → VAL (0.19338490264070418, 0.19230498529844742, 0.19343355704697987, 0.1898292671889895) | TEST (0.18693333333333334, 0.1847153107300893, 0.1869333333333333, 0.18308630918433647)\n",
      "KBest | hog | MLP → VAL (0.1675113363563617, 0.1644673765505787, 0.16754720357941838, 0.16472682727312138) | TEST (0.16933333333333334, 0.167932399427915, 0.16933333333333334, 0.16711560209770468)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:31<00:00, 327.91it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:11<00:00, 327.87it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:11<00:00, 326.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA | sift | SVM → VAL (0.3019471859162443, 0.2965584900270573, 0.30195973154362415, 0.29496141585127506) | TEST (0.2936, 0.293053414128187, 0.2936, 0.28941021409590145)\n",
      "PCA | sift | RandomForest → VAL (0.23019471859162444, 0.22448338382269395, 0.23017807606263982, 0.22191772430614448) | TEST (0.2272, 0.2227943549011901, 0.22719999999999999, 0.22035506856874038)\n",
      "PCA | sift | MLP → VAL (0.25606828487596694, 0.2525284188825261, 0.2560733780760626, 0.25164908790087975) | TEST (0.23866666666666667, 0.2385806795966235, 0.23866666666666667, 0.2375066808323777)\n",
      "KBest | sift | SVM → VAL (0.2795412109895972, 0.2732238204169349, 0.27954541387024606, 0.27257367073423194) | TEST (0.2808, 0.2805076524325657, 0.2808, 0.2768984049758962)\n",
      "KBest | sift | RandomForest → VAL (0.2328620965590824, 0.2220992484957962, 0.23285369127516778, 0.2230359717251616) | TEST (0.22773333333333334, 0.2242214496336504, 0.22773333333333334, 0.22174061640884574)\n",
      "KBest | sift | MLP → VAL (0.25073352894105094, 0.24436153745885045, 0.25075615212527963, 0.24509005036894016) | TEST (0.24266666666666667, 0.2374401424553637, 0.24266666666666667, 0.23824744088233982)\n"
     ]
    }
   ],
   "source": [
    "#Part2\n",
    "\n",
    "print(\"\\n=== PART 2: FEATURE REDUCTION (PCA & SelectKBest) ===\")\n",
    "for feat in features_to_try:\n",
    "    X_train_feat = extract_features(X_train, feat)\n",
    "    X_val_feat   = extract_features(X_val,   feat)\n",
    "    X_test_feat  = extract_features(X_test,  feat)\n",
    "\n",
    "    # -------- 1. scale BEFORE dimensionality-reduction --------\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train_feat)\n",
    "    X_val_scaled   = scaler.transform(X_val_feat)\n",
    "    X_test_scaled  = scaler.transform(X_test_feat)\n",
    "\n",
    "    n_comp = min(100, X_train_scaled.shape[1])\n",
    "\n",
    "    # -- PCA --\n",
    "    X_train_pca, X_val_pca, pca = reduce_features(\n",
    "        'pca', X_train_scaled, X_val_scaled, n_components=n_comp)\n",
    "    X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "    for name, base_model in models_to_try.items():\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train_pca, y_train)\n",
    "        val_metrics  = evaluate(model, X_val_pca,  y_val)\n",
    "        test_metrics = evaluate(model, X_test_pca, y_test)\n",
    "        print(f\"PCA | {feat} | {name} → VAL {val_metrics} | TEST {test_metrics}\")\n",
    "\n",
    "    # -- SelectKBest --\n",
    "    X_train_kb, X_val_kb, kb = reduce_features(\n",
    "        'selectkbest', X_train_scaled, X_val_scaled, y_train, n_components=n_comp)\n",
    "    X_test_kb = kb.transform(X_test_scaled)\n",
    "\n",
    "    for name, base_model in models_to_try.items():\n",
    "        model = clone(base_model)\n",
    "        model.fit(X_train_kb, y_train)\n",
    "        val_metrics  = evaluate(model, X_val_kb,  y_val)\n",
    "        test_metrics = evaluate(model, X_test_kb, y_test)\n",
    "        print(f\"KBest | {feat} | {name} → VAL {val_metrics} | TEST {test_metrics}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb811f49",
   "metadata": {},
   "source": [
    "### Part 2\n",
    "\n",
    "| Model | Feature    | Reduction | Split | Acc        | Prec   | Rec    | F1     |\n",
    "| ----- | ---------- | --------- | ----- | ---------- | ------ | ------ | ------ |\n",
    "| SVM   | color hist | PCA       | VAL   | 0.4556     | 0.4689 | 0.4556 | 0.4565 |\n",
    "| SVM   | color hist | PCA       | TEST  | 0.4672     | 0.4846 | 0.4672 | 0.4700 |\n",
    "| SVM   | color hist | KBest     | VAL   | 0.3878     | 0.4176 | 0.3878 | 0.3936 |\n",
    "| SVM   | color hist | KBest     | TEST  | 0.3864     | 0.4168 | 0.3864 | 0.3924 |\n",
    "| SVM   | HOG        | PCA       | VAL   | 0.3673     | 0.3681 | 0.3673 | 0.3641 |\n",
    "| SVM   | HOG        | PCA       | TEST  | 0.3525     | 0.3533 | 0.3525 | 0.3486 |\n",
    "| SVM   | HOG        | KBest     | VAL   | 0.2083     | 0.2074 | 0.2084 | 0.2041 |\n",
    "| SVM   | HOG        | KBest     | TEST  | 0.2059     | 0.2072 | 0.2059 | 0.2016 |\n",
    "| SVM   | SIFT       | PCA       | VAL   | 0.3019     | 0.2966 | 0.3020 | 0.2950 |\n",
    "| SVM   | SIFT       | PCA       | TEST  | 0.2936     | 0.2931 | 0.2936 | 0.2894 |\n",
    "| SVM   | SIFT       | KBest     | VAL   | 0.2795     | 0.2732 | 0.2795 | 0.2726 |\n",
    "| SVM   | SIFT       | KBest     | TEST  | 0.2808     | 0.2805 | 0.2808 | 0.2769 |\n",
    "| RF    | color hist | PCA       | VAL   | 0.7959     | 0.7986 | 0.7959 | 0.7964 |\n",
    "| RF    | color hist | PCA       | TEST  | 0.7827     | 0.7851 | 0.7827 | 0.7829 |\n",
    "| RF    | color hist | KBest     | VAL   | 0.7967     | 0.8006 | 0.7967 | 0.7975 |\n",
    "| RF    | color hist | KBest     | TEST  | **0.7859** | 0.7890 | 0.7859 | 0.7863 |\n",
    "| RF    | HOG        | PCA       | VAL   | 0.2910     | 0.2896 | 0.2911 | 0.2871 |\n",
    "| RF    | HOG        | PCA       | TEST  | 0.2677     | 0.2649 | 0.2677 | 0.2624 |\n",
    "| RF    | HOG        | KBest     | VAL   | 0.1934     | 0.1923 | 0.1934 | 0.1898 |\n",
    "| RF    | HOG        | KBest     | TEST  | 0.1869     | 0.1847 | 0.1869 | 0.1831 |\n",
    "| RF    | SIFT       | PCA       | VAL   | 0.2302     | 0.2245 | 0.2302 | 0.2219 |\n",
    "| RF    | SIFT       | PCA       | TEST  | 0.2272     | 0.2228 | 0.2272 | 0.2204 |\n",
    "| RF    | SIFT       | KBest     | VAL   | 0.2329     | 0.2221 | 0.2329 | 0.2230 |\n",
    "| RF    | SIFT       | KBest     | TEST  | 0.2277     | 0.2242 | 0.2277 | 0.2217 |\n",
    "| MLP   | color hist | PCA       | VAL   | 0.6191     | 0.6234 | 0.6191 | 0.6187 |\n",
    "| MLP   | color hist | PCA       | TEST  | 0.5933     | 0.5946 | 0.5933 | 0.5922 |\n",
    "| MLP   | color hist | KBest     | VAL   | 0.5399     | 0.5443 | 0.5399 | 0.5405 |\n",
    "| MLP   | color hist | KBest     | TEST  | 0.5451     | 0.5514 | 0.5451 | 0.5470 |\n",
    "| MLP   | HOG        | PCA       | VAL   | 0.2905     | 0.2857 | 0.2905 | 0.2854 |\n",
    "| MLP   | HOG        | PCA       | TEST  | 0.2792     | 0.2763 | 0.2792 | 0.2744 |\n",
    "| MLP   | HOG        | KBest     | VAL   | 0.1675     | 0.1645 | 0.1675 | 0.1647 |\n",
    "| MLP   | HOG        | KBest     | TEST  | 0.1693     | 0.1679 | 0.1693 | 0.1671 |\n",
    "| MLP   | SIFT       | PCA       | VAL   | 0.2561     | 0.2525 | 0.2561 | 0.2516 |\n",
    "| MLP   | SIFT       | PCA       | TEST  | 0.2387     | 0.2386 | 0.2387 | 0.2375 |\n",
    "| MLP   | SIFT       | KBest     | VAL   | 0.2507     | 0.2444 | 0.2508 | 0.2451 |\n",
    "| MLP   | SIFT       | KBest     | TEST  | 0.2427     | 0.2374 | 0.2427 | 0.2382 |\n",
    "\n",
    "**Best Part 2 result**\n",
    "`RandomForest + color histogram + KBest` – **Test Acc 0.7859**, Prec 0.7890, Rec 0.7859, F1 0.7863.\n",
    "\n",
    "**Comments**\n",
    "\n",
    "* Dimensionality reduction **helps none** of the colour-hist models reach Part 1’s peak, but KBest retains more signal than PCA.\n",
    "* For HOG/SIFT, reductions sometimes give tiny gains for linear SVM but generally remain low.\n",
    "* Trees suffer when PCA scrambles colour-bin semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caab83c3",
   "metadata": {},
   "source": [
    "### Bonus: Training with all extracted features at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b1a90cf",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ALL FEATURES COMBINED ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████| 30000/30000 [00:02<00:00, 12853.91it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3749/3749 [00:00<00:00, 12496.63it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 3750/3750 [00:00<00:00, 12395.79it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:23<00:00, 360.89it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:10<00:00, 363.53it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:10<00:00, 362.36it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████| 30000/30000 [01:34<00:00, 317.26it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3749/3749 [00:11<00:00, 326.96it/s]\n",
      "100%|█████████████████████████████████████████████████████████████████████████████| 3750/3750 [00:12<00:00, 311.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ALL | SVM → VAL (0.5222726060282742, 0.5281307982236062, 0.5222872483221477, 0.5225300534365175) | TEST (0.5152, 0.5211580813079298, 0.5152, 0.5149782806843413)\n",
      "ALL | RandomForest → VAL (0.6497732728727661, 0.6522608164710885, 0.649786129753915, 0.6477732555419344) | TEST (0.6464, 0.647913621599414, 0.6464, 0.6431685982738545)\n",
      "ALL | MLP → VAL (0.5038676980528141, 0.5046204590431036, 0.5038944071588367, 0.503565808948682) | TEST (0.5056, 0.5067068877027162, 0.5056, 0.5055219705923668)\n"
     ]
    }
   ],
   "source": [
    "#PART-1 BONUS: train / test on ALL features at once\n",
    "print(\"\\n=== ALL FEATURES COMBINED ===\")\n",
    "\n",
    "feat_arrays = {f: {\n",
    "        'train': extract_features(X_train, f),\n",
    "        'val'  : extract_features(X_val,   f),\n",
    "        'test' : extract_features(X_test,  f)}\n",
    "    for f in features_to_try}\n",
    "\n",
    "X_train_all = np.hstack([feat_arrays[f]['train'] for f in features_to_try])\n",
    "X_val_all   = np.hstack([feat_arrays[f]['val']   for f in features_to_try])\n",
    "X_test_all  = np.hstack([feat_arrays[f]['test']  for f in features_to_try])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_all = scaler.fit_transform(X_train_all)\n",
    "X_val_all   = scaler.transform(X_val_all)\n",
    "X_test_all  = scaler.transform(X_test_all)\n",
    "\n",
    "for name, base_model in models_to_try.items():\n",
    "    model = clone(base_model)\n",
    "    model.fit(X_train_all, y_train)\n",
    "\n",
    "    val_metrics  = evaluate(model, X_val_all,  y_val)\n",
    "    test_metrics = evaluate(model, X_test_all, y_test)\n",
    "\n",
    "    print(f\"ALL | {name}\"\n",
    "          f\" → VAL {val_metrics} | TEST {test_metrics}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5780451",
   "metadata": {},
   "source": [
    "### Bonus - All features concatenated\n",
    "\n",
    "| Model | Split | Acc        | Prec   | Rec    | F1     |\n",
    "| ----- | ----- | ---------- | ------ | ------ | ------ |\n",
    "| SVM   | VAL   | 0.5223     | 0.5281 | 0.5223 | 0.5225 |\n",
    "| SVM   | TEST  | 0.5152     | 0.5212 | 0.5152 | 0.5150 |\n",
    "| RF    | VAL   | 0.6498     | 0.6523 | 0.6498 | 0.6478 |\n",
    "| RF    | TEST  | **0.6464** | 0.6479 | 0.6464 | 0.6432 |\n",
    "| MLP   | VAL   | 0.5039     | 0.5046 | 0.5039 | 0.5036 |\n",
    "| MLP   | TEST  | 0.5056     | 0.5067 | 0.5056 | 0.5055 |\n",
    "\n",
    "**Best Bonus result**\n",
    "`RandomForest (all features)` – **Test Acc 0.6464**, Prec 0.6479, Rec 0.6464, F1 0.6432.\n",
    "\n",
    "**Comment**\n",
    "Concatenating heterogeneous features boosts SVM a bit but dilutes RF/MLP compared with colour-hist alone; mixed-scale noise outweighs the added signal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0d0230",
   "metadata": {},
   "source": [
    "### Comparison of Part 1, 2 and Bonus\n",
    "\n",
    "### SVM\n",
    "\n",
    "| Setting                     | Test Acc   |\n",
    "| --------------------------- | ---------- |\n",
    "| Part 1 – color hist         | 0.4891     |\n",
    "| Part 1 – HOG                | 0.3523     |\n",
    "| Part 1 – SIFT               | 0.2971     |\n",
    "| Part 2 – color hist + PCA   | 0.4672     |\n",
    "| Part 2 – color hist + KBest | 0.3864     |\n",
    "| Part 2 – HOG + PCA          | 0.3525     |\n",
    "| Part 2 – HOG + KBest        | 0.2059     |\n",
    "| Part 2 – SIFT + PCA         | 0.2936     |\n",
    "| Part 2 – SIFT + KBest       | 0.2808     |\n",
    "| Bonus – all features        | **0.5152** |\n",
    "\n",
    "### Random Forest\n",
    "\n",
    "| Setting                     | Test Acc   |\n",
    "| --------------------------- | ---------- |\n",
    "| Part 1 – color hist         | **0.8547** |\n",
    "| Part 1 – HOG                | 0.2536     |\n",
    "| Part 1 – SIFT               | 0.2416     |\n",
    "| Part 2 – color hist + PCA   | 0.7827     |\n",
    "| Part 2 – color hist + KBest | 0.7859     |\n",
    "| Part 2 – HOG + PCA          | 0.2677     |\n",
    "| Part 2 – HOG + KBest        | 0.1869     |\n",
    "| Part 2 – SIFT + PCA         | 0.2272     |\n",
    "| Part 2 – SIFT + KBest       | 0.2277     |\n",
    "| Bonus – all features        | 0.6464     |\n",
    "\n",
    "### MLP\n",
    "\n",
    "| Setting                     | Test Acc   |\n",
    "| --------------------------- | ---------- |\n",
    "| Part 1 – color hist         | **0.7296** |\n",
    "| Part 1 – HOG                | 0.2272     |\n",
    "| Part 1 – SIFT               | 0.2488     |\n",
    "| Part 2 – color hist + PCA   | 0.5933     |\n",
    "| Part 2 – color hist + KBest | 0.5451     |\n",
    "| Part 2 – HOG + PCA          | 0.2792     |\n",
    "| Part 2 – HOG + KBest        | 0.1693     |\n",
    "| Part 2 – SIFT + PCA         | 0.2387     |\n",
    "| Part 2 – SIFT + KBest       | 0.2427     |\n",
    "| Bonus – all features        | 0.5056     |\n",
    "\n",
    "---\n",
    "\n",
    "### General comments\n",
    "\n",
    "* **Colour histogram + RandomForest (Part 1)** is the overall champion (85 % accuracy).\n",
    "* Dimensionality reduction **rarely beats** the raw feature when the raw feature is already informative and compact (colour hist).\n",
    "* **Feature fusion** (Bonus) helps SVM a little but harms RF/MLP—highlighting the need for careful normalisation/selection when merging heterogeneous descriptors.\n",
    "* Future gains likely require hyper-parameter tuning, better encodings for HOG/SIFT (e.g., BoVW/VLAD), or moving to deep features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf9839b",
   "metadata": {},
   "source": [
    "## PART 3, 4, 5\n",
    "\n",
    "For Part 3 & 4, we used resnet18, mobilenet_v3_small, shufflenet_v2_x0_5 (with IMAGENET1K_V1 weights for part 3).\n",
    "For Part 5, we tried these models\n",
    "\n",
    "| Model                 | Convolutional backbone                                                                                                                             | Head                                                    |\n",
    "| --------------------- | -------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------- |\n",
    "| **custom\\_small**     | 3 blocks → <br>• Conv 3×3 @32 → BN → ReLU → MaxPool 2×2 <br>• Conv 3×3 @64 → BN → ReLU → MaxPool 2×2 <br>• Conv 3×3 @128 → BN → ReLU → MaxPool 2×2 | Global-Avg-Pool → Dropout 0.30 → FC 256 → ReLU → FC 25  |\n",
    "| **custom\\_medium**    | 4 blocks (filters 32 → 64 → 128 → 256) – each: Conv 3×3 → BN → ReLU → MaxPool 2×2                                                                  | Global-Avg-Pool → Dropout 0.30 → FC 512 → ReLU → FC 25  |\n",
    "| **custom\\_deep\\_bn**  | 5 blocks (32 / 64 / 128 / 256 / 512) with BN after every conv                                                                                      | Global-Avg-Pool → Dropout 0.40 → FC 512 → ReLU → FC 25  |\n",
    "| **custom\\_ultra\\_bn** | 7 blocks (32 / 64 / 128 / 256 / 512 / 512 / 512) – deeper & wider, BN on all layers                                                                | Global-Avg-Pool → Dropout 0.50 → FC 1024 → ReLU → FC 25 |\n",
    "\n",
    "*All conv layers use padding = 1, keeping spatial size before each 2×2 pooling step.*\n",
    "\n",
    "\n",
    "We used Adams optimizer with 0.0001 learning rate .\n",
    "We resized the dataset to 224x224 but used all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d769f7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data for part3\n",
    "data_transforms = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"Birds_25/train\", transform=data_transforms)\n",
    "valid_dataset = datasets.ImageFolder(\"Birds_25/valid\", transform=data_transforms)\n",
    "\n",
    "# Split valid into val and test\n",
    "val_size = len(valid_dataset) // 2\n",
    "test_size = len(valid_dataset) - val_size\n",
    "val_dataset, test_dataset = random_split(\n",
    "    valid_dataset,\n",
    "    [val_size, test_size],\n",
    "    generator=torch.Generator().manual_seed(42)\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97517735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3&4&5 funtions\n",
    "def fine_tune(model_name, train_loader, val_loader, num_classes, pretrained=True):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    weights = 'IMAGENET1K_V1' if pretrained else None\n",
    "\n",
    "    if model_name == \"resnet18\":\n",
    "        model = models.resnet18(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    elif model_name == \"mobilenet_v3_small\":\n",
    "        model = models.mobilenet_v3_small(weights=weights)\n",
    "        model.classifier[3] = nn.Linear(model.classifier[3].in_features, num_classes)\n",
    "    elif model_name == \"shufflenet_v2_x0_5\":\n",
    "        model = models.shufflenet_v2_x0_5(weights=weights)\n",
    "        model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model name: {model_name}\")\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    val_accs, val_precs, val_recs = [], [], []\n",
    "    best_loss = float('inf')\n",
    "    \n",
    "    best_path = f\"{model_name}_{'pretrained' if pretrained else 'scratch'}_best.pth\"\n",
    "    \n",
    "    patience, trigger = 5, 0  #early stopping\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # ------- TRAIN -------\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "\n",
    "        for inputs, labels in tqdm(train_loader,\n",
    "                                   desc=f'E{epoch+1:02d} ▸ train',\n",
    "                                   leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(inputs), labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = running_loss / len(train_loader)\n",
    "        train_losses.append(avg_train_loss)\n",
    "\n",
    "        # ------- VALIDATE -------\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(val_loader,\n",
    "                                       desc=f'E{epoch+1:02d} ▸ val  ',\n",
    "                                       leave=False):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_loss += criterion(model(inputs), labels).item()\n",
    "\n",
    "        avg_val_loss = val_loss / len(val_loader)\n",
    "\n",
    "        # one pass to get metrics\n",
    "        _, acc, prec, rec, _ = evaluate_loader(model, val_loader, criterion)\n",
    "\n",
    "        val_losses.append(avg_val_loss)\n",
    "        val_accs.append(acc)\n",
    "        val_precs.append(prec)\n",
    "        val_recs.append(rec)\n",
    "\n",
    "        print(f'Epoch {epoch+1}: Train Loss: {avg_train_loss:.4f}  |  Val Loss: {avg_val_loss:.4f}')\n",
    "\n",
    "        # ------- EARLY-STOP -------\n",
    "        if avg_val_loss < best_loss:\n",
    "            best_loss = avg_val_loss\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "            trigger = 0\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                print('Early stopping')\n",
    "                break\n",
    "\n",
    "\n",
    "    # ------- LOSS CURVE --------\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(train_losses, label='Train loss')\n",
    "    plt.plot(val_losses,   label='Val loss')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Loss'); plt.legend()\n",
    "    plt.title(f'{model_name} - loss')\n",
    "    plt.savefig(f\"{model_name}_{'pretrained' if pretrained else 'scratch'}_loss.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # ------- METRIC CURVES -------\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(val_accs,  label='Accuracy')\n",
    "    plt.plot(val_precs, label='Precision')\n",
    "    plt.plot(val_recs,  label='Recall')\n",
    "    plt.xlabel('Epoch'); plt.ylabel('Score'); plt.legend()\n",
    "    plt.title(f'{model_name} - val metrics')\n",
    "    plt.savefig(f\"{model_name}_{'pretrained' if pretrained else 'scratch'}_metrics.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    \n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_loader(\n",
    "            model, test_loader, criterion)\n",
    "\n",
    "    print(f\"[{model_name} | {'pre' if pretrained else 'scratch'}] \"\n",
    "          f\"TEST  Loss={test_loss:.4f}  \"\n",
    "          f\"Acc={test_acc:.4f}  \"\n",
    "          f\"P={test_prec:.4f}  R={test_rec:.4f}  F1={test_f1:.4f}\")\n",
    "\n",
    "    \n",
    "    \n",
    "def evaluate_loader(model, loader, criterion):\n",
    "    model.eval()\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    tot_loss, y_true, y_pred = 0.0, [], []\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            out = model(x)\n",
    "            tot_loss += criterion(out, y).item()\n",
    "\n",
    "            y_true.extend(y.cpu().numpy())\n",
    "            y_pred.extend(out.argmax(1).cpu().numpy())\n",
    "\n",
    "    acc  = accuracy_score (y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, average='macro', zero_division=0)\n",
    "    rec  = recall_score   (y_true, y_pred, average='macro', zero_division=0)\n",
    "    f1   = f1_score       (y_true, y_pred, average='macro', zero_division=0)\n",
    "\n",
    "    return tot_loss / len(loader), acc, prec, rec, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d210042a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training resnet18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.3919  |  Val Loss: 0.1469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.0644  |  Val Loss: 0.1028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.0288  |  Val Loss: 0.1064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0293  |  Val Loss: 0.0851\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0239  |  Val Loss: 0.1165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0268  |  Val Loss: 0.1056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0177  |  Val Loss: 0.1356\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0187  |  Val Loss: 0.1054\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0148  |  Val Loss: 0.1348\n",
      "Early stopping\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resnet18 | pre] TEST  Loss=0.0940  Acc=0.9699  P=0.9704  R=0.9701  F1=0.9700\n",
      "\n",
      "Training mobilenet_v3_small\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 0.8392  |  Val Loss: 0.2190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 0.2002  |  Val Loss: 0.1334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.1167  |  Val Loss: 0.0996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.0718  |  Val Loss: 0.1030\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.0477  |  Val Loss: 0.0792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.0374  |  Val Loss: 0.0839\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.0284  |  Val Loss: 0.0757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.0240  |  Val Loss: 0.0899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.0191  |  Val Loss: 0.0789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0178  |  Val Loss: 0.0795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v3_small | pre] TEST  Loss=0.0724  Acc=0.9771  P=0.9776  R=0.9771  F1=0.9772\n",
      "\n",
      "Training shufflenet_v2_x0_5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.4090  |  Val Loss: 1.3623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.0178  |  Val Loss: 0.6807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.6196  |  Val Loss: 0.4783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.4677  |  Val Loss: 0.3927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.3778  |  Val Loss: 0.3406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.3202  |  Val Loss: 0.3108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.2776  |  Val Loss: 0.2859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.2460  |  Val Loss: 0.2656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.2161  |  Val Loss: 0.2490\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.1873  |  Val Loss: 0.2442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2_x0_5 | pre] TEST  Loss=0.2472  Acc=0.9211  P=0.9227  R=0.9217  F1=0.9217\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Part3\n",
    "for model_name in [\"resnet18\", \"mobilenet_v3_small\", \"shufflenet_v2_x0_5\"]:\n",
    "    print(f\"\\nTraining {model_name}\")\n",
    "    fine_tune(model_name, train_loader, val_loader, num_classes=25, pretrained=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8bc8b73",
   "metadata": {},
   "source": [
    "### Part 3 Results\n",
    "\n",
    "| Model                  | Best val loss | Test acc   | Fit pattern                                                                                              | Key note                                                                                                   |\n",
    "| ---------------------- | ------------- | ---------- | -------------------------------------------------------------------------------------------------------- | ---------------------------------------------------------------------------------------------------------- |\n",
    "| **ResNet-18**          | ≈ 0.085       | **97.0 %** | **Slight over-fit** – train loss dives < 0.03 while val loss oscillates 0.08–0.14; early-stop at epoch 9 | Good accuracy, but gap hints at mild memorisation; would benefit from lower LR or stronger regularisation. |\n",
    "| **MobileNet V3-small** | **0.075**     | **97.7 %** | **Well-balanced** – train & val curves fall together and flatten; acc/prec/rec rise steadily to ≈ 0.98   | Best accuracy-to-size ratio; curves show stable convergence with minimal over-fit.                         |\n",
    "| **ShuffleNet V2 0.5×** | 0.24          | 92.1 %     | **Under-fit** – losses drop smoothly but remain higher; small gap between train & val                    | Lightweight backbone needs more epochs or wider (1.0×) variant to reach parity.                            |\n",
    "\n",
    "#### Observations\n",
    "\n",
    "* **Transfer-learning effectiveness:** All three networks start far below ImageNet head loss; pre-trained features are reused successfully.\n",
    "* **Capacity vs. data size:** ResNet (≈ 11 M params) fits quickly, even risks over-fit; ShuffleNet (＜ 1 M) struggles to model the 25-class bird dataset in only 10 epochs; MobileNet (≈ 2.5 M) hits the sweet spot.\n",
    "* **Metric consistency:** Accuracy, precision and recall curves almost overlap for every model → class balance is good and no single class dominates performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2874f261",
   "metadata": {},
   "source": [
    "### Loss And Metric Graphs For Part 3\n",
    "\n",
    "Resnet18:\n",
    "\n",
    "<img src=\"resnet18_pretrained_loss.png\" alt=\"resnet18 loss\" width=\"400\">\n",
    "\n",
    "<img src=\"resnet18_pretrained_metrics.png\" alt=\"resnet18 loss\" width=\"400\">\n",
    "\n",
    "MobileNet V3-small:\n",
    "\n",
    "<img src=\"mobilenet_v3_small_pretrained_loss.png\" alt=\"mobilenet loss\" width=\"400\">\n",
    "\n",
    "<img src=\"mobilenet_v3_small_pretrained_metrics.png\" alt=\"mobilenet loss\" width=\"400\">\n",
    "\n",
    "ShuffleNet V2 0.5×:\n",
    "\n",
    "<img src=\"shufflenet_v2_x0_5_pretrained_loss.png\" alt=\"shufflenet loss\" width=\"400\">\n",
    "\n",
    "<img src=\"shufflenet_v2_x0_5_pretrained_metrics.png\" alt=\"shufflenet loss\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0c889f",
   "metadata": {},
   "source": [
    "## PART 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3568ca98",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training resnet18 from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 1.9645  |  Val Loss: 1.5323\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 1.1455  |  Val Loss: 1.0387\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 0.7901  |  Val Loss: 0.9047\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 0.5786  |  Val Loss: 0.7053\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 0.4149  |  Val Loss: 0.6535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 0.3051  |  Val Loss: 0.6652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 0.2089  |  Val Loss: 0.6484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 0.1362  |  Val Loss: 0.7337\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.1125  |  Val Loss: 0.7550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.0940  |  Val Loss: 0.9196\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resnet18 | scratch] TEST  Loss=0.6337  Acc=0.8152  P=0.8362  R=0.8169  F1=0.8162\n",
      "\n",
      "Training mobilenet_v3_small from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.5286  |  Val Loss: 2.2155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 2.0527  |  Val Loss: 1.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 1.8016  |  Val Loss: 1.7066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 1.6163  |  Val Loss: 1.5688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.4543  |  Val Loss: 1.4868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.3221  |  Val Loss: 1.4160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.2022  |  Val Loss: 1.3614\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.0937  |  Val Loss: 1.2594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 0.9948  |  Val Loss: 1.2955\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 0.9055  |  Val Loss: 1.2785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[mobilenet_v3_small | scratch] TEST  Loss=1.2697  Acc=0.5981  P=0.6108  R=0.5989  F1=0.6016\n",
      "\n",
      "Training shufflenet_v2_x0_5 from scratch\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss: 2.8826  |  Val Loss: 2.6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss: 2.5423  |  Val Loss: 2.4117\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss: 2.3069  |  Val Loss: 2.1829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss: 2.1206  |  Val Loss: 2.0291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss: 1.9813  |  Val Loss: 1.8946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss: 1.8739  |  Val Loss: 1.8138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss: 1.7726  |  Val Loss: 1.7310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss: 1.6870  |  Val Loss: 1.6778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss: 1.6168  |  Val Loss: 1.6248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10: Train Loss: 1.5450  |  Val Loss: 1.5723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\3261586911.py:106: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[shufflenet_v2_x0_5 | scratch] TEST  Loss=1.6068  Acc=0.4829  P=0.4892  R=0.4849  F1=0.4801\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x400 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Part4\n",
    "for model_name in [\"resnet18\", \"mobilenet_v3_small\", \"shufflenet_v2_x0_5\"]:\n",
    "    print(f\"\\nTraining {model_name} from scratch\")\n",
    "    fine_tune(model_name, train_loader, val_loader, num_classes=25, pretrained=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9651a2b5",
   "metadata": {},
   "source": [
    "### Part 4 (vs Part 3)\n",
    "\n",
    "| Model                  | **Pre-trained (P3)**            | **Scratch (P4)**          | Δ Accuracy | Curve behaviour                                                                                                                                                                                |\n",
    "| ---------------------- | ------------------------------- | ------------------------- | ---------- | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **ResNet-18**          | **97 %** acc, val loss ≈ 0.10   | 81 % acc, val loss ≈ 0.65 | **-16 pp** | P3: drops to 0.02 train loss by epoch 3, small val oscillations → slight over-fit.<br>P4: steady descent, then plateau; gap widens after epoch 6 → capacity starts to over-fit noisy features. |\n",
    "| **MobileNet V3-small** | **97.7 %** acc, val loss ≈ 0.08 | 59 % acc, val loss ≈ 1.3  | **-39 pp** | P3: train & val curves almost overlap; smooth ↑ to 0.98 acc.<br>P4: linear fall in loss but curves still > 1.0 at epoch 10; metrics climb only to ≈ 0.6 ⇒ under-fit.                           |\n",
    "| **ShuffleNet V2-0.5×** | **92 %** acc, val loss ≈ 0.25   | 48 % acc, val loss ≈ 1.6  | **-44 pp** | P3: slow but monotonic improvement to 0.92 acc.<br>P4: both losses fall but remain high; metrics < 0.5 even after 10 epochs ⇒ clear under-fit.                                                 |\n",
    "\n",
    "#### Key observations\n",
    "\n",
    "* **Transfer learning payoff**\n",
    "\n",
    "  * Pre-trained weights give a **+16 – 44 pp** boost in test accuracy and reach useful performance in < 3 epochs (ResNet, MobileNet).\n",
    "  * From-scratch models are still learning basic features after 10 epochs; MobileNet & ShuffleNet haven’t converged.\n",
    "\n",
    "* **Capacity vs. data size**\n",
    "\n",
    "  * Our bird dataset (25 classes, 1.5 k images each) is large enough to fine-tune, but **not large enough to train deep CNNs from random init** without many more epochs/regularisation.\n",
    "  * Smaller backbones (ShuffleNet-0.5×) suffer most: insufficient representational power plus no pre-learned features.\n",
    "\n",
    "* **Curve patterns**\n",
    "\n",
    "  * **Pre-trained**: rapid drop in both train & val loss, early stabilisation; accuracy / precision / recall curves almost identical → balanced class performance.\n",
    "  * **Scratch**: losses descend slowly; val curves lag behind train, signalling under-fit at same epoch budget. ResNet eventually starts over-fitting (val loss ticks up after epoch 7).\n",
    "\n",
    "*(All metrics are macro-averaged over 25 classes.)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10fa162",
   "metadata": {},
   "source": [
    "### Loss And Metric Graphs For Part 4\n",
    "\n",
    "Resnet18:\n",
    "\n",
    "<img src=\"resnet18_scratch_loss.png\" alt=\"resnet18 loss\" width=\"400\">\n",
    "\n",
    "<img src=\"resnet18_scratch_metrics.png\" alt=\"resnet18 loss\" width=\"400\">\n",
    "\n",
    "MobileNet V3-small:\n",
    "\n",
    "<img src=\"mobilenet_v3_small_scratch_loss.png\" alt=\"mobilenet loss\" width=\"400\">\n",
    "\n",
    "<img src=\"mobilenet_v3_small_scratch_metrics.png\" alt=\"mobilenet loss\" width=\"400\">\n",
    "\n",
    "ShuffleNet V2 0.5×:\n",
    "\n",
    "<img src=\"shufflenet_v2_x0_5_scratch_loss.png\" alt=\"shufflenet loss\" width=\"400\">\n",
    "\n",
    "<img src=\"shufflenet_v2_x0_5_scratch_metrics.png\" alt=\"shufflenet loss\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aba2fbb",
   "metadata": {},
   "source": [
    "## PART 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4eb409fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions\n",
    "class CustomCNN(nn.Module):\n",
    "    def __init__(self, cfg, num_classes):\n",
    "        super().__init__()\n",
    "        in_c = 3\n",
    "        layers = []\n",
    "        for f in cfg[\"filters\"]:\n",
    "            layers += [\n",
    "                nn.Conv2d(in_c, f, 3, padding=1),\n",
    "                nn.BatchNorm2d(f) if cfg.get(\"batch_norm\", True) else nn.Identity(),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool2d(2)\n",
    "            ]\n",
    "            in_c = f\n",
    "        self.features = nn.Sequential(*layers)\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        fc_in = in_c\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(cfg.get(\"dropout\", 0.0)),\n",
    "            nn.Linear(fc_in, cfg.get(\"fc\", 256)),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(cfg.get(\"fc\", 256), num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.avgpool(x).flatten(1)\n",
    "        return self.classifier(x)\n",
    "\n",
    "\n",
    "def train_custom(cfg, train_loader, val_loader, test_loader, num_classes):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = CustomCNN(cfg, num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_losses, val_losses = [], []\n",
    "    val_accs, val_precs, val_recs = [], [], []\n",
    "\n",
    "    best_loss, trigger, patience = float(\"inf\"), 0, 5\n",
    "    tag = cfg[\"name\"]\n",
    "    best_path = f\"{tag}_best.pth\"\n",
    "\n",
    "    for epoch in range(10):\n",
    "        # ----- train -----\n",
    "        model.train()\n",
    "        tl = 0.0\n",
    "        for x, y in tqdm(train_loader, desc=f\"E{epoch+1:02d} ▸ train\", leave=False):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(x), y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            tl += loss.item()\n",
    "        train_losses.append(tl / len(train_loader))\n",
    "\n",
    "        # ----- validate loss -----\n",
    "        model.eval()\n",
    "        vl = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x, y in tqdm(val_loader, desc=f\"E{epoch+1:02d} ▸ val  \", leave=False):\n",
    "                x, y = x.to(device), y.to(device)\n",
    "                vl += criterion(model(x), y).item()\n",
    "        vl /= len(val_loader)\n",
    "        val_losses.append(vl)\n",
    "\n",
    "        # ----- validate metrics -----\n",
    "        _, acc, prec, rec, _ = evaluate_loader(model, val_loader, criterion)\n",
    "        val_accs.append(acc)\n",
    "        val_precs.append(prec)\n",
    "        val_recs.append(rec)\n",
    "\n",
    "        print(f\"[{tag}] epoch {epoch+1:02d}  train={train_losses[-1]:.4f}  \"\n",
    "              f\"val={vl:.4f}  acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}\")\n",
    "\n",
    "        if vl < best_loss:\n",
    "            best_loss, trigger = vl, 0\n",
    "            torch.save(model.state_dict(), best_path)\n",
    "        else:\n",
    "            trigger += 1\n",
    "            if trigger >= patience:\n",
    "                print(f\"[{tag}] early stop\")\n",
    "                break\n",
    "\n",
    "    # ----- loss curve -----\n",
    "    plt.plot(train_losses, label=\"train\")\n",
    "    plt.plot(val_losses, label=\"val\")\n",
    "    plt.title(f\"{tag} loss\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.savefig(f\"{tag}_loss.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # ----- metric curves -----\n",
    "    plt.plot(val_accs, label=\"accuracy\")\n",
    "    plt.plot(val_precs, label=\"precision\")\n",
    "    plt.plot(val_recs, label=\"recall\")\n",
    "    plt.title(f\"{tag} metrics\")\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"score\")\n",
    "    plt.savefig(f\"{tag}_metrics.png\")\n",
    "    plt.clf()\n",
    "\n",
    "    # ----- test -----\n",
    "    model.load_state_dict(torch.load(best_path))\n",
    "    test_loss, test_acc, test_prec, test_rec, test_f1 = evaluate_loader(model, test_loader, criterion)\n",
    "    print(f\"[{tag}] TEST loss={test_loss:.4f}  acc={test_acc:.4f}  \"\n",
    "          f\"P={test_prec:.4f}  R={test_rec:.4f}  F1={test_f1:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4b614aa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- Part 5 ----------\n",
    "cfg_list = [\n",
    "    {\"name\": \"custom_small\",   \"filters\": [32, 64, 128],           \"fc\": 256, \"dropout\": 0.3},\n",
    "    {\"name\": \"custom_medium\",  \"filters\": [32, 64, 128, 256],     \"fc\": 512, \"dropout\": 0.3},\n",
    "    {\"name\": \"custom_deep_bn\", \"filters\": [32, 64, 128, 256,512], \"fc\": 512, \"dropout\": 0.4},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "91da784c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** training custom_small ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 01  train=2.9397  val=2.6309  acc=0.2285  prec=0.2427  rec=0.2248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 02  train=2.5771  val=2.4408  acc=0.2536  prec=0.2747  rec=0.2531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 03  train=2.4395  val=2.3067  acc=0.3040  prec=0.3391  rec=0.3020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 04  train=2.3657  val=2.3223  acc=0.2872  prec=0.3028  rec=0.2844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 05  train=2.3085  val=2.1957  acc=0.3328  prec=0.3495  rec=0.3310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 06  train=2.2611  val=2.1355  acc=0.3403  prec=0.3541  rec=0.3378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 07  train=2.2260  val=2.0981  acc=0.3595  prec=0.3646  rec=0.3567\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 08  train=2.1909  val=2.0456  acc=0.3685  prec=0.3808  rec=0.3657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 09  train=2.1538  val=2.0199  acc=0.3883  prec=0.3963  rec=0.3859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] epoch 10  train=2.1315  val=1.9978  acc=0.3915  prec=0.4036  rec=0.3893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\1417699975.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_small] TEST loss=2.0293  acc=0.3619  P=0.3732  R=0.3633  F1=0.3599\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n*** training {cfg_list[0]['name']} ***\")\n",
    "train_custom(cfg_list[0], train_loader, val_loader, test_loader, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6bc51af9",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** training custom_medium ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 01  train=2.6670  val=2.3232  acc=0.3000  prec=0.3139  rec=0.2985\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 02  train=2.2600  val=2.1127  acc=0.3552  prec=0.3711  rec=0.3546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 03  train=2.1232  val=2.0084  acc=0.3739  prec=0.4285  rec=0.3701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 04  train=2.0020  val=1.9408  acc=0.3947  prec=0.4429  rec=0.3924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 05  train=1.9211  val=1.7956  acc=0.4403  prec=0.4521  rec=0.4367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 06  train=1.8473  val=1.7407  acc=0.4600  prec=0.4631  rec=0.4566\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 07  train=1.7834  val=1.7187  acc=0.4675  prec=0.4868  rec=0.4657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 08  train=1.7392  val=1.6346  acc=0.4848  prec=0.4960  rec=0.4810\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 09  train=1.6836  val=1.6016  acc=0.4923  prec=0.5028  rec=0.4896\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] epoch 10  train=1.6278  val=1.5059  acc=0.5269  prec=0.5407  rec=0.5246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\1417699975.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_medium] TEST loss=1.5295  acc=0.5208  P=0.5369  R=0.5221  F1=0.5105\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n*** training {cfg_list[1]['name']} ***\")\n",
    "train_custom(cfg_list[1], train_loader, val_loader, test_loader, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d0244a4",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "*** training custom_deep_bn ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 01  train=2.4326  val=2.0471  acc=0.3619  prec=0.3689  rec=0.3590\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 02  train=1.9252  val=1.7928  acc=0.4432  prec=0.4875  rec=0.4390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 03  train=1.7023  val=1.7530  acc=0.4491  prec=0.5001  rec=0.4441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 04  train=1.5246  val=1.4821  acc=0.5464  prec=0.5953  rec=0.5448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 05  train=1.3906  val=1.3485  acc=0.5757  prec=0.6484  rec=0.5745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 06  train=1.2682  val=1.1804  acc=0.6416  prec=0.6694  rec=0.6383\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 07  train=1.1601  val=1.2162  acc=0.6307  prec=0.6867  rec=0.6302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 08  train=1.0791  val=0.9996  acc=0.6925  prec=0.7157  rec=0.6897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 09  train=1.0029  val=0.9492  acc=0.7152  prec=0.7402  rec=0.7129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] epoch 10  train=0.9267  val=1.0547  acc=0.6792  prec=0.7262  rec=0.6779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\1417699975.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_deep_bn] TEST loss=0.9136  acc=0.7181  P=0.7439  R=0.7197  F1=0.7182\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"\\n*** training {cfg_list[2]['name']} ***\")\n",
    "train_custom(cfg_list[2], train_loader, val_loader, test_loader, num_classes=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "583a0129",
   "metadata": {},
   "outputs": [],
   "source": [
    "ultra_cfg = {\n",
    "    \"name\": \"custom_ultra_bn\",\n",
    "    \"filters\": [32, 64, 128, 256, 512, 512, 512],\n",
    "    \"fc\": 1024,\n",
    "    \"dropout\": 0.5,\n",
    "    \"batch_norm\": True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "334b224c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 01  train=1.9295  val=1.2776  acc=0.5907  prec=0.6141  rec=0.5871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 02  train=1.0813  val=0.9085  acc=0.7147  prec=0.7380  rec=0.7143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 03  train=0.7451  val=0.7303  acc=0.7736  prec=0.8052  rec=0.7724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 04  train=0.5487  val=0.5490  acc=0.8328  prec=0.8362  rec=0.8324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 05  train=0.4013  val=0.6539  acc=0.7925  prec=0.8310  rec=0.7927\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 06  train=0.2968  val=0.5982  acc=0.8261  prec=0.8451  rec=0.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 07  train=0.2174  val=0.6359  acc=0.8256  prec=0.8394  rec=0.8251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 08  train=0.1738  val=0.4475  acc=0.8733  prec=0.8783  rec=0.8736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 09  train=0.1374  val=0.5176  acc=0.8581  prec=0.8675  rec=0.8593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] epoch 10  train=0.1129  val=0.4929  acc=0.8693  prec=0.8804  rec=0.8692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ali19\\AppData\\Local\\Temp\\ipykernel_14940\\1417699975.py:108: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(best_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[custom_ultra_bn] TEST loss=0.4049  acc=0.8819  P=0.8884  R=0.8817  F1=0.8832\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_custom(ultra_cfg, train_loader, val_loader, test_loader, num_classes=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea514658",
   "metadata": {},
   "source": [
    "### Part 5\n",
    "\n",
    "| Model                 | Params\\* | Best Val Acc | Test Acc | Convergence notes                                                                                                                             |\n",
    "| --------------------- | -------- | ------------ | -------- | --------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| **custom\\_small**     | ≈ 0.5 M  | 0.39         | 0.36     | Very slow learning; loss still > 2.0 after 10 epochs. Under-capacity for 25-class task.                                                       |\n",
    "| **custom\\_medium**    | ≈ 1.5 M  | 0.53         | 0.52     | Steady but shallow improvement; begins to capture some class structure.                                                                       |\n",
    "| **custom\\_deep\\_bn**  | ≈ 3.5 M  | 0.71         | 0.72     | Depth+BN clearly help: val loss falls below 1.0, metrics reach mid-70 %. Minor over-fit after epoch 9.                                        |\n",
    "| **custom\\_ultra\\_bn** | ≈ 6 M    | **0.87**     | **0.88** | Adds two extra 512-channel blocks & bigger FC. Rapid drop to < 0.5 val loss, peaks \\~88 % macro metrics. Best trade-off inside custom family. |\n",
    "\n",
    "\\*rough estimate from channel sizes.\n",
    "\n",
    "#### Key observations\n",
    "\n",
    "* **Capacity drives accuracy** – every extra block lifts val accuracy by \\~15 pp until “ultra”, which surpasses 85 %.\n",
    "* **Batch-norm + deeper stacks** are crucial; without them (small / medium) training is slow and under-fits.\n",
    "* **custom\\_ultra\\_bn vs. pretrained baselines**\n",
    "\n",
    "  * Still below MobileNet V3-small pretrained (≈ 98 %) but **closes half the gap** to ImageNet transfer with only in-domain learning.\n",
    "  * Shows that a purpose-built, moderately deep network can approach good accuracy when transfer weights are unavailable.\n",
    "  \n",
    "  \n",
    "### Loss And Metric Graphs For Part 5\n",
    "\n",
    "Small:\n",
    "\n",
    "<img src=\"custom_small_loss.png\" alt=\"small\" width=\"400\">\n",
    "\n",
    "<img src=\"custom_small_metrics_metrics.png\" alt=\"small\" width=\"400\">\n",
    "\n",
    "Medium:\n",
    "\n",
    "<img src=\"custom_medium_loss.png\" alt=\"med\" width=\"400\">\n",
    "\n",
    "<img src=\"custom_medium_metrics.png\" alt=\"med\" width=\"400\">\n",
    "\n",
    "Deep:\n",
    "\n",
    "<img src=\"custom_deep_bn_loss.png\" alt=\"deep\" width=\"400\">\n",
    "\n",
    "<img src=\"custom_deep_bn_metrics.png\" alt=\"deep\" width=\"400\">\n",
    "\n",
    "Ultra Deep:\n",
    "\n",
    "<img src=\"custom_ultra_bn_loss.png\" alt=\"ultra\" width=\"400\">\n",
    "\n",
    "<img src=\"custom_ultra_bn_metrics_metrics.png\" alt=\"ultra\" width=\"400\">\n",
    "\n",
    "\n",
    "  \n",
    "# OVERALL\n",
    "\n",
    "Across the project we moved from classic computer-vision pipelines to deep learning and saw a clear accuracy ladder:\n",
    "\n",
    "1. **Hand-crafted features (Parts 1 & 2):** colour-histograms with a Random Forest peaked at \\~85 % test accuracy; all other feature/model combos stayed ≤ 73 %. Dimensionality-reduction or feature fusion never beat the raw colour-hist baseline.\n",
    "\n",
    "2. **Pre-trained CNNs (Part 3):** fine-tuning ImageNet models skyrocketed performance - MobileNet V3-small reached \\~98 % accuracy with minimal training.\n",
    "\n",
    "3. **Same CNNs from scratch (Part 4):** without transfer learning accuracy fell sharply (ResNet-18 ≈ 81 %, MobileNet ≈ 60 %, ShuffleNet ≈ 48 %), confirming the value of pre-training.\n",
    "\n",
    "4. **Custom CNNs (Part 5):** progressively deeper scratch-built nets improved from 36 % (small) to 88 % (ultra). Our “custom\\_ultra\\_bn” narrowed the gap to pre-trained models but still trailed MobileNet by \\~10 pp.\n",
    "\n",
    "**Bottom line:** colour histograms + trees are strong for shallow methods, but pre-trained deep networks remain unbeatable; deep models trained from scratch need substantial depth (and likely more data / augmentation) to compete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85cbdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
